# Backend - Hexagonal Architecture (Ports and Adapters)

## ğŸ“ Architettura Esagonale

Questo backend Ã¨ stato completamente ristrutturato seguendo l'**Architettura Esagonale** (Hexagonal Architecture / Ports and Adapters), un pattern architetturale che garantisce:

âœ… **Separazione chiara delle responsabilitÃ **  
âœ… **TestabilitÃ  completa** (domain indipendente da framework)  
âœ… **FlessibilitÃ ** (sostituisci facilmente infrastruttura)  
âœ… **Indipendenza dai dettagli tecnici** (il core non dipende da FastAPI, httpx, etc.)

---

## ğŸ—ï¸ Struttura del Progetto

```
backend-hexagonal/
â”œâ”€â”€ domain/                      # ğŸ”µ CORE - Logica di Business
â”‚   â”œâ”€â”€ models/                  # EntitÃ  del dominio
â”‚   â”‚   â”œâ”€â”€ text_document.py    # TextDocument
â”‚   â”‚   â””â”€â”€ llm_result.py        # LLMResult, ResultStatus, ResultCode
â”‚   â”œâ”€â”€ services/                # Servizi di dominio
â”‚   â”‚   â””â”€â”€ text_processor.py   # TextProcessorService
â”‚   â””â”€â”€ exceptions.py            # Eccezioni del dominio
â”‚
â”œâ”€â”€ application/                 # ğŸŸ¢ USE CASES
â”‚   â”œâ”€â”€ use_cases/               # Casi d'uso (logica applicativa)
â”‚   â”‚   â”œâ”€â”€ summarize_text.py
â”‚   â”‚   â”œâ”€â”€ improve_text.py
â”‚   â”‚   â”œâ”€â”€ translate_text.py
â”‚   â”‚   â”œâ”€â”€ analyze_six_hats.py
â”‚   â”‚   â””â”€â”€ generate_text.py
â”‚   â””â”€â”€ ports/                   # INTERFACCE (contratti)
â”‚       â”œâ”€â”€ input/               # Primary Ports (driving)
â”‚       â”‚   â””â”€â”€ text_processor_port.py
â”‚       â””â”€â”€ output/              # Secondary Ports (driven)
â”‚           â”œâ”€â”€ llm_provider_port.py
â”‚           â”œâ”€â”€ prompt_builder_port.py
â”‚           â””â”€â”€ response_parser_port.py
â”‚
â”œâ”€â”€ adapters/                    # ğŸŸ¡ ADATTATORI (implementazioni)
â”‚   â”œâ”€â”€ input/                   # Primary Adapters (driving)
â”‚   â”‚   â””â”€â”€ fastapi_adapter.py  # API REST
â”‚   â””â”€â”€ output/                  # Secondary Adapters (driven)
â”‚       â”œâ”€â”€ llm_client_adapter.py      # Implementazione LLM con streaming
â”‚       â”œâ”€â”€ prompt_builder_adapter.py  # Costruzione prompt
â”‚       â””â”€â”€ json_parser_adapter.py     # Parsing JSON
â”‚
â”œâ”€â”€ infrastructure/              # ğŸ”´ Infrastruttura
â”‚   â”œâ”€â”€ config.py               # Configurazione (Pydantic Settings)
â”‚   â””â”€â”€ di_container.py         # Dependency Injection
â”‚
â”œâ”€â”€ main.py                     # Entry point
â””â”€â”€ requirements.txt
```

---

## ğŸ¯ Principi Architetturali

### 1. Domain (Core)
**Il cuore dell'applicazione**, contiene la logica di business pura:
- **Modelli**: `TextDocument`, `LLMResult`
- **Servizi**: Orchestrazione dei use cases
- **Eccezioni**: Errori specifici del dominio
- âŒ **NON dipende** da FastAPI, httpx, o altri framework

### 2. Application (Use Cases)
**Logica applicativa**, indipendente dai dettagli tecnici:
- **Use Cases**: Un use case per ogni operazione (Summarize, Improve, Translate, Six Hats, Generate)
- **Ports**: Interfacce (contratti) per comunicare con l'esterno
  - **Input Ports**: Come il mondo esterno chiama il nostro core
  - **Output Ports**: Come il nostro core chiama il mondo esterno

### 3. Adapters (Implementazioni)
**Implementazioni concrete** delle interfacce:
- **Input Adapters**: FastAPI (ma potrebbe essere CLI, GraphQL, gRPC...)
- **Output Adapters**: LLM Client, Prompt Builder, JSON Parser

### 4. Infrastructure
**Configurazione e wiring**:
- Settings (da .env)
- Dependency Injection Container

---

## ğŸ”Œ Ports and Adapters

### Primary Ports (Input)
```python
class ITextProcessor(ABC):
    async def summarize(document, percentage) -> LLMResult
    async def improve(document, criterion) -> LLMResult
    async def translate(document, target_language) -> LLMResult
    async def analyze_six_hats(document, hat) -> LLMResult
    async def generate(prompt) -> LLMResult
```

### Secondary Ports (Output)
```python
class ILLMProvider(ABC):
    async def generate_completion(messages, model, temperature) -> str
    
class IPromptBuilder(ABC):
    def build_summarize_prompt(document, percentage) -> List[Dict]
    
class IResponseParser(ABC):
    def parse_response(raw_response) -> LLMResult
```

---

## ğŸ“Š Flusso di una Richiesta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST /llm/summarize
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Adapter (Input)    â”‚  â† Input Adapter
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ text_processor.summarize()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TextProcessorService        â”‚  â† Domain Service
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ use_case.execute()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SummarizeTextUseCase        â”‚  â† Use Case
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
   â”‚          â”‚            â”‚
   â”‚          â”‚            â–¼
   â”‚          â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚          â”‚       â”‚ LLMClient       â”‚ â† Output Adapter
   â”‚          â”‚       â”‚ (con streaming) â”‚
   â”‚          â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚          â”‚                â”‚
   â”‚          â–¼                â–¼
   â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚     â”‚  Prompt   â”‚    â”‚  External  â”‚
   â”‚     â”‚  Builder  â”‚    â”‚  LLM API   â”‚
   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    JSON     â”‚ â† Output Adapter
â”‚   Parser    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ LLMResult
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â† Response
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Setup e Avvio

### 1. Prerequisiti
```bash
Python 3.12.x
```

### 2. Crea Virtual Environment
```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

### 3. Installa Dipendenze
```bash
pip install -r requirements.txt
```

### 4. Configura `.env`
Crea un file `.env` nella root:
```env
LLM_API_URL=http://your-llm-endpoint:port/v1/chat/completions
LLM_MODEL=your-model-name
LLM_API_KEY=your-api-key
```

### 5. Avvia il Server
```bash
# Opzione 1: Direttamente con Python
python main.py

# Opzione 2: Con uvicorn (reload automatico)
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Server disponibile su: `http://localhost:8000`

---

## ğŸ§ª Testing

### Testare il Domain (Unit Tests)
```python
# test_summarize_use_case.py
import pytest
from unittest.mock import Mock, AsyncMock

@pytest.mark.asyncio
async def test_summarize_success():
    # Mock delle dipendenze
    llm_provider = Mock()
    llm_provider.generate_completion = AsyncMock(return_value='{"outcome": ...}')
    
    prompt_builder = Mock()
    response_parser = Mock()
    
    # Use case
    use_case = SummarizeTextUseCase(
        llm_provider, prompt_builder, response_parser, "model"
    )
    
    # Esecuzione
    document = TextDocument(content="Test text")
    result = await use_case.execute(document, 30)
    
    # Assertions
    assert result.is_successful()
```

### Testare gli Adapters
Puoi testare gli adapter in isolamento sostituendo le dipendenze con mock.

---

## ğŸ“ Endpoints API

### 1. POST /llm/summarize
Riassume un testo
```json
{
  "text": "Testo da riassumere...",
  "percentage": 30
}
```

### 2. POST /llm/improve
Migliora un testo
```json
{
  "text": "Testo da migliorare...",
  "criterion": "chiarezza e stile professionale"
}
```

### 3. POST /llm/translate
Traduce un testo
```json
{
  "text": "Text to translate...",
  "targetLanguage": "Italian"
}
```

### 4. POST /llm/six-hats
Analizza con sei cappelli
```json
{
  "text": "Testo da analizzare...",
  "hat": "bianco"
}
```

Cappelli disponibili: `bianco`, `rosso`, `nero`, `giallo`, `verde`, `blu`

### 5. POST /llm/generate
Genera testo da prompt
```json
{
  "prompt": "Scrivi una storia su..."
}
```

### 6. GET /health
Health check
```json
{
  "status": "ok",
  "message": "Server is awake!",
  "architecture": "hexagonal"
}
```

---

## âœ… Vantaggi dell'Architettura Esagonale

### 1. TestabilitÃ 
```python
# Testa il domain SENZA bisogno di FastAPI o LLM reale
result = await use_case.execute(document, 30)
assert result.is_successful()
```

### 2. SostituibilitÃ 
Puoi sostituire qualsiasi adapter senza toccare il core:
- FastAPI â†’ CLI / GraphQL / gRPC
- LLM Provider â†’ MockLLM per testing
- JSON Parser â†’ XML Parser / YAML Parser

### 3. Indipendenza
Il domain NON conosce:
- FastAPI
- httpx
- pydantic (tranne per Settings)
- Nessun dettaglio tecnico

### 4. ManutenibilitÃ 
Ogni layer ha responsabilitÃ  chiare:
- Domain â†’ Business logic
- Application â†’ Use cases
- Adapters â†’ Implementazioni tecniche
- Infrastructure â†’ Configurazione

---

## ğŸ”„ Migrazione dal Vecchio Backend

### Vecchia Struttura
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ client.py
â”‚   â”‚   â”œâ”€â”€ parser.py
â”‚   â”‚   â”œâ”€â”€ prompts.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ main.py
```

### Mapping alla Nuova Struttura

| Vecchio | Nuovo | Layer |
|---------|-------|-------|
| `llm/schemas.py` | `domain/models/llm_result.py` | Domain |
| `llm/client.py` | `adapters/output/llm_client_adapter.py` | Adapter |
| `llm/parser.py` | `adapters/output/json_parser_adapter.py` | Adapter |
| `llm/prompts.py` | `adapters/output/prompt_builder_adapter.py` | Adapter |
| `main.py` endpoints | `application/use_cases/*.py` | Use Cases |
| `main.py` FastAPI | `adapters/input/fastapi_adapter.py` | Adapter |
| `config.py` | `infrastructure/config.py` | Infrastructure |

---

## ğŸ†• FunzionalitÃ  Mantenute

âœ… **Streaming LLM** - Supporto completo per risposte streaming  
âœ… **Fallback automatico** - Passa al fallback LLM in caso di errore  
âœ… **Gestione disconnessione** - Annulla chiamate se il client si disconnette  
âœ… **CORS configurabile** - Origini configurabili via env  
âœ… **5 Endpoint** - Tutti e 5 gli endpoint funzionanti (incluso `/llm/generate`)  
âœ… **Validazione sicurezza** - Protezione anti-prompt-injection nei prompt

---

## ğŸ“š Ulteriori Estensioni

### Aggiungere un nuovo Use Case
1. Crea `application/use_cases/new_use_case.py`
2. Aggiungi metodo in `ITextProcessor`
3. Implementa in `TextProcessorService`
4. Aggiungi endpoint in `fastapi_adapter.py`

### Aggiungere un nuovo Adapter
1. Definisci l'interfaccia in `application/ports/output/`
2. Implementa in `adapters/output/`
3. Registra nel `DIContainer`

### Aggiungere un nuovo Input Adapter (es. CLI)
1. Crea `adapters/input/cli_adapter.py`
2. Usa `text_processor` dal `DIContainer`
3. Nessuna modifica al domain necessaria!

---

## ğŸ“ Supporto

Per domande sull'architettura o implementazione:
- Consulta i commenti nei file sorgente
- Ogni classe ha docstring dettagliate
- I port hanno definizioni chiare delle interfacce

---

**Versione**: 2.0.0 (Hexagonal Architecture)  
**Data**: Febbraio 2026  
**Pattern**: Ports and Adapters (Hexagonal Architecture)
